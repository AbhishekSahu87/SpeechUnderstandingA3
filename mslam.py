# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hrZKik7W_hf9aqj5oylYwt35SLOSB475
"""

!pip install transformers datasets peft evaluate torchaudio
from huggingface_hub import notebook_login
notebook_login()

!wget https://dl.fbaipublicfiles.com/covost/covost_v2.en_de.tsv.tar.gz
!tar -xvzf /content/covost_v2.en_de.tsv.tar.gz -C /content/cvdata

!wget https://dl.fbaipublicfiles.com/covost/covost_v2.en_de.tsv.tar.gz
!tar -xvzf /content/covost_v2.en_de.tsv.tar.gz -C /content/cvdata/cv-corpus/covost2

# Reproduce Results on Paperâ€™s Datasets
# CoVoST-2 Speech Translation with Whisper
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from datasets import load_dataset
import evaluate

# Load model and dataset
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
processor = WhisperProcessor.from_pretrained("openai/whisper-small", language="en", task="transcribe")

data_dir = "/content/cvdata/cv-corpus/covost2"

covost = load_dataset(
    "covost2",
    "en_de",
    data_dir=data_dir,
    data_files={
        "train": f"{data_dir}/covost_v2.en_de.tsv"
    }
)

# Preprocess
def preprocess(batch):
    audio = batch["audio"]
    inputs = processor(audio["array"], sampling_rate=audio["sampling_rate"], text=batch["translation"], return_tensors="pt")
    return inputs

dataset = covost.map(preprocess, remove_columns=covost["train"].column_names)
trainer = Seq2SeqTrainer(
    model=model,
    args=Seq2SeqTrainingArguments(output_dir="covost-whisper", per_device_train_batch_size=8, learning_rate=1e-5),
    train_dataset=dataset["train"],
    tokenizer=processor.tokenizer,
)
trainer.train()

# Evaluate BLEU
metric = evaluate.load("sacrebleu")
preds = trainer.predict(dataset["test"])
decoded_preds = processor.batch_decode(preds.predictions, skip_special_tokens=True)
print(metric.compute(predictions=decoded_preds, references=[[ref] for ref in dataset["test"]["translation"]]))

# Fleurs-LangID with Wav2Vec2-XLS-R

from transformers import AutoFeatureExtractor, AutoModelForAudioClassification

model = AutoModelForAudioClassification.from_pretrained("facebook/wav2vec2-xls-r-300m", num_labels=102)
fleurs = load_dataset("google/fleurs", "en_us")

# Preprocess
def preprocess(batch):
    inputs = feature_extractor(batch["audio"]["array"], sampling_rate=16_000, return_tensors="pt")
    return inputs

dataset = fleurs.map(preprocess, batched=True, remove_columns=["audio", "transcription"])
trainer = Trainer(
    model=model,
    args=TrainingArguments(output_dir="fleurs-langid", per_device_train_batch_size=16, learning_rate=3e-5),
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
)
trainer.train()
print(trainer.evaluate())

# Common Voice ASR + LoRA Fine-Tuning
from peft import LoraConfig, get_peft_model

# Apply LoRA to Whisper
config = LoraConfig(r=8, lora_alpha=16, target_modules=["k_proj", "v_proj", "q_proj"])
model = get_peft_model(model, config)

# Load and split Common Voice
common_voice = load_dataset("mozilla-foundation/common_voice_11_0", "en", split="train+validation").train_test_split(test_size=0.2)

# Train
trainer = Seq2SeqTrainer(
    model=model,
    args=Seq2SeqTrainingArguments(output_dir="cv-lora", per_device_train_batch_size=8, learning_rate=1e-4),
    train_dataset=common_voice["train"],
    eval_dataset=common_voice["test"],
    tokenizer=processor.tokenizer,
)
trainer.train()

# Evaluate WER
wer = evaluate.load("wer")
preds = trainer.predict(common_voice["test"])
decoded_preds = processor.batch_decode(preds.predictions, skip_special_tokens=True)
print(f"WER: {wer.compute(predictions=decoded_preds, references=common_voice['test']['sentence'])}")

# DoRA Layers
import torch
import torch.nn as nn
from peft.tuners.lora import Linear

class DoRALayer(nn.Module):
    def __init__(self, base_layer, rank=8, alpha=16):
        super().__init__()
        self.base_layer = base_layer
        self.lora_A = nn.Linear(base_layer.in_features, rank, bias=False)
        self.lora_B = nn.Linear(rank, base_layer.out_features, bias=False)
        self.m = nn.Parameter(torch.ones(base_layer.out_features))  # Magnitude vector

        # Initialize LoRA weights
        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B.weight)

        # Freeze original weights
        for param in self.base_layer.parameters():
            param.requires_grad = False

    def forward(self, x):
        delta_W = self.lora_B(self.lora_A(x))  # Low-rank directional update
        W_dir = self.base_layer.weight + delta_W
        W_dir_norm = torch.norm(W_dir, dim=1, keepdim=True)
        W_scaled = self.m * (W_dir / W_dir_norm)  # Decomposed weight
        return torch.nn.functional.linear(x, W_scaled, self.base_layer.bias)

# DoRA to Whisper CoVoST-2 Translation
from transformers import WhisperForConditionalGeneration

# Load model
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")

# Replace target layers with DoRA
for layer in model.model.decoder.layers:
    layer.self_attn.q_proj = DoRALayer(layer.self_attn.q_proj, rank=8)
    layer.self_attn.k_proj = DoRALayer(layer.self_attn.k_proj, rank=8)
    layer.self_attn.v_proj = DoRALayer(layer.self_attn.v_proj, rank=8)

# trainable parameters
print(f"Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad)}")

# Fine-Tune on CoVoST-2 with DoRA

from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer

# Training arguments
args = Seq2SeqTrainingArguments(
    output_dir="covost-dora",
    per_device_train_batch_size=8,
    learning_rate=1e-4,
    num_train_epochs=5,
)

# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=args,
    train_dataset=tokenized_covost["train"],
    test_dataset=tokenized_covost["test"],
    tokenizer=processor.tokenizer,
)
trainer.train()

predictions = trainer.predict(test_dataset)
pred_texts = processor.batch_decode(predictions.predictions, skip_special_tokens=True)
labels = processor.batch_decode(predictions.label_ids, skip_special_tokens=True)
bleu_score = metric.compute(predictions=pred_texts, references=[[ref] for ref in labels])
print(f"BLEU: {bleu_score['score']:.1f}")

# Apply DoRA to Wav2Vec2-XLS-R (Fleurs-LangID)
from transformers import Wav2Vec2ForSequenceClassification

model = Wav2Vec2ForSequenceClassification.from_pretrained("facebook/wav2vec2-xls-r-300m", num_labels=102)

# Replace final classification layer with DoRA
model.classifier = DoRALayer(nn.Linear(model.config.hidden_size, 102), rank=8)

#  Fine-Tune on Fleurs-LangID
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="fleurs-dora",
    per_device_train_batch_size=16,
    learning_rate=3e-5,
    num_train_epochs=10,
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_fleurs["train"],
    test_dataset=tokenized_fleurs["test"],
)
trainer.train()

results = trainer.evaluate()
print(f"Validation Accuracy: {results['eval_accuracy']}")

# DoRA to Whisper for ASR
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
for layer in model.model.decoder.layers:
    layer.self_attn.q_proj = DoRALayer(layer.self_attn.q_proj, rank=8)
    layer.self_attn.k_proj = DoRALayer(layer.self_attn.k_proj, rank=8)
    layer.self_attn.v_proj = DoRALayer(layer.self_attn.v_proj, rank=8)

# Train on Common Voice
trainer = Seq2SeqTrainer(
    model=model,
    args=Seq2SeqTrainingArguments(output_dir="cv-dora", learning_rate=1e-4),
    train_dataset=common_voice["train"],
    test_dataset=common_voice["test"],
    tokenizer=processor.tokenizer,
)
trainer.train()

wer_metric = evaluate.load("wer")
predictions = trainer.predict(test_dataset)
pred_texts = processor.batch_decode(predictions.predictions, skip_special_tokens=True)
true_texts = processor.batch_decode(predictions.label_ids, skip_special_tokens=True)
wer = wer_metric.compute(predictions=pred_texts, references=true_texts)
print(f"WER: {wer * 100:.2f}%")  # Explicit print